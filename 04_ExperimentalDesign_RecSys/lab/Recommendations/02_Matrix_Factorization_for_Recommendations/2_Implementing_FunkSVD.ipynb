{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing FunkSVD - Solution\n",
    "\n",
    "In this notebook we will take a look at writing our own function that performs FunkSVD, which will follow the steps you saw in the previous video.  If you find that you aren't ready to tackle this task on your own, feel free to skip to the following video where you can watch as I walk through the steps.\n",
    "\n",
    "To test our algorithm, we will run it on the subset of the data you worked with earlier.  Run the cell below to get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 7. 10.  8.]\n",
      " [ 6. 10.  7.]\n",
      " [ 8.  9.  8.]\n",
      " [ 8. 10. 10.]\n",
      " [ 9.  9.  9.]\n",
      " [ 8.  9.  9.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import sparse\n",
    "import svd_tests as t\n",
    "%matplotlib inline\n",
    "\n",
    "# Read in the datasets\n",
    "movies = pd.read_csv('movies_clean.csv')\n",
    "reviews = pd.read_csv('reviews_clean.csv')\n",
    "\n",
    "del movies['Unnamed: 0']\n",
    "del reviews['Unnamed: 0']\n",
    "\n",
    "# Create user-by-item matrix\n",
    "user_items = reviews[['user_id', 'movie_id', 'rating', 'timestamp']]\n",
    "user_by_movie = user_items.groupby(['user_id', 'movie_id'])['rating'].max().unstack()\n",
    "\n",
    "# Create data subset\n",
    "user_movie_subset = user_by_movie[[75314,  68646, 99685]].dropna(axis=0)\n",
    "ratings_mat = np.matrix(user_movie_subset)\n",
    "print(ratings_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35479, 35)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100001, 5)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8022, 13850)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_by_movie.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`1.` You will use the **user_movie_subset** matrix to show that your FunkSVD algorithm will converge.  In the below cell, use the comments and document string to assist you as you complete writing your own function to complete FunkSVD.  You may also want to try to complete the funtion on your own without the assistance of comments.  You may feel free to remove and add to the function in any way that gets you a working solution! \n",
    "\n",
    "**Notice:** There isn't a sigma matrix in this version of matrix factorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FunkSVD(ratings_mat, latent_features=4, learning_rate=0.0001, iters=100):\n",
    "    '''\n",
    "    This function performs matrix factorization using a basic form of FunkSVD with no regularization\n",
    "    \n",
    "    INPUT:\n",
    "    ratings_mat - (numpy array) a matrix with users as rows, movies as columns, and ratings as values\n",
    "    latent_features - (int) the number of latent features used\n",
    "    learning_rate - (float) the learning rate \n",
    "    iters - (int) the number of iterations\n",
    "    \n",
    "    OUTPUT:\n",
    "    user_mat - (numpy array) a user by latent feature matrix\n",
    "    movie_mat - (numpy array) a latent feature by movie matrix\n",
    "    '''\n",
    "    \n",
    "    # Set up useful values to be used through the rest of the function\n",
    "    n_users = ratings_mat.shape[0]\n",
    "    n_movies = ratings_mat.shape[1]\n",
    "    n_nan = np.sum(np.sum(np.isnan(ratings_mat), axis=0), axis=1)[0,0]\n",
    "    num_ratings = n_users*n_movies - n_nan\n",
    "    # Alternative:\n",
    "    # num_ratings = np.count_nonzero(~np.isnan(ratings_mat))\n",
    "    \n",
    "    # Initialize the user and movie matrices with random values\n",
    "    # https://numpy.org/doc/stable/reference/random/generated/numpy.random.rand.html\n",
    "    user_mat = np.random.rand(n_users,latent_features)\n",
    "    movie_mat = np.random.rand(latent_features, n_movies)\n",
    "    \n",
    "    # Initialize sse at 0 for first iteration\n",
    "    sse_accum = 0\n",
    "    \n",
    "    print(\"Optimizaiton Statistics\")\n",
    "    print(\"Iterations | Mean Squared Error \")\n",
    "    \n",
    "    for it in range(iters):\n",
    "        # Update our sse\n",
    "        old_sse = sse_accum\n",
    "        sse_accum = 0\n",
    "        \n",
    "        for i in range(n_users):\n",
    "            for j in range(n_movies):\n",
    "                r_ij = ratings_mat[i,j]\n",
    "                if not np.isnan(r_ij):\n",
    "                    u_i = user_mat[i,:]\n",
    "                    v_j = movie_mat[:,j]\n",
    "                    # Compute the error as the actual minus the dot product\n",
    "                    # of the user and movie latent features\n",
    "                    e_ij = (r_ij - np.dot(u_i,v_j))**2\n",
    "                    # Keep track of the sum of squared errors for the matrix\n",
    "                    sse_accum += e_ij                    \n",
    "                    # Update the values in each matrix in the direction of the gradient\n",
    "                    # dE/du_i = -2(a_ij - u_i*v_j)*v_j\n",
    "                    # dE/dv_j = -2(a_ij - u_i*v_j)*u_i\n",
    "                    # x_new <- x_old - alpha * dE/dx\n",
    "                    u_new = u_i + 2.0*learning_rate*(r_ij - np.dot(u_i,v_j))*v_j\n",
    "                    v_new = v_j + 2.0*learning_rate*(r_ij - np.dot(u_i,v_j))*u_new\n",
    "                    user_mat[i,:] = u_new\n",
    "                    movie_mat[:,j] = v_new\n",
    "\n",
    "        print(f\"{it} | {sse_accum}\")\n",
    "\n",
    "    return user_mat, movie_mat "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`2.` Try out your function on the **user_movie_subset** dataset.  First try 3 latent features, a learning rate of 0.005, and 10 iterations.  When you take the dot product of the resulting U and V matrices, how does the resulting **user_movie** matrix compare to the original subset of the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizaiton Statistics\n",
      "Iterations | Mean Squared Error \n",
      "0 | 908.2933277741536\n",
      "1 | 632.688661950179\n",
      "2 | 349.6213932965372\n",
      "3 | 148.9564205968025\n",
      "4 | 53.43853287844017\n",
      "5 | 21.252235396322945\n",
      "6 | 12.617523935110881\n",
      "7 | 10.461291427242324\n",
      "8 | 9.85106047356348\n",
      "9 | 9.611582433318963\n"
     ]
    }
   ],
   "source": [
    "# Use your function with 3 latent features, lr of 0.005 and 10 iterations\n",
    "user_mat, movie_mat = FunkSVD(ratings_mat=ratings_mat,\n",
    "                              latent_features=3,\n",
    "                              learning_rate=0.005,\n",
    "                              iters=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 7.58093691  9.16365197  8.46181382]\n",
      " [ 7.07800734  8.56994439  7.56590017]\n",
      " [ 7.46303981  9.0111772   8.40785591]\n",
      " [ 8.53221956 10.31502179  9.34586598]\n",
      " [ 8.12259308  9.81721833  8.89875982]\n",
      " [ 7.76777163  9.38681644  8.48406098]]\n",
      "[[ 7. 10.  8.]\n",
      " [ 6. 10.  7.]\n",
      " [ 8.  9.  8.]\n",
      " [ 8. 10. 10.]\n",
      " [ 9.  9.  9.]\n",
      " [ 8.  9.  9.]]\n"
     ]
    }
   ],
   "source": [
    "print(np.dot(user_mat, movie_mat))\n",
    "print(ratings_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_**Task: compare the predicted vs actual ratings and write your findings here**_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`3.` Let's try out the function again on the **user_movie_subset** dataset.  This time we will again use 3 latent features and a learning rate of 0.005.  However, let's bump up the number of iterations to 300.  When you take the dot product of the resulting U and V matrices, how does the resulting **user_movie** matrix compare to the original subset of the data?  What do you notice about the `mean squared error` at the end of each training iteration?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizaiton Statistics\n",
      "Iterations | Mean Squared Error \n",
      "0 | 982.9294445147943\n",
      "1 | 725.3239802034384\n",
      "2 | 432.6206599833486\n",
      "3 | 198.1108852757119\n",
      "4 | 72.40283087056682\n",
      "5 | 26.161688112480093\n",
      "6 | 13.321420576256218\n",
      "7 | 10.238289816148885\n",
      "8 | 9.471403192089301\n",
      "9 | 9.214078687403543\n",
      "10 | 9.074754791850333\n",
      "11 | 8.9679671289896\n",
      "12 | 8.872205517671059\n",
      "13 | 8.780519574912647\n",
      "14 | 8.69008958204111\n",
      "15 | 8.5996172112697\n",
      "16 | 8.508465807695835\n",
      "17 | 8.416304519266156\n",
      "18 | 8.322945139243085\n",
      "19 | 8.228265869945036\n",
      "20 | 8.132176364678838\n",
      "21 | 8.034602372591658\n",
      "22 | 7.935479522021477\n",
      "23 | 7.834751242450377\n",
      "24 | 7.732368478385877\n",
      "25 | 7.628290117713156\n",
      "26 | 7.522483654192936\n",
      "27 | 7.414925879482509\n",
      "28 | 7.3056035241114134\n",
      "29 | 7.194513820318364\n",
      "30 | 7.0816649808692596\n",
      "31 | 6.967076594875442\n",
      "32 | 6.850779942538773\n",
      "33 | 6.732818229460717\n",
      "34 | 6.613246739347958\n",
      "35 | 6.492132902332649\n",
      "36 | 6.369556274960837\n",
      "37 | 6.245608427255456\n",
      "38 | 6.1203927321163025\n",
      "39 | 5.994024052635174\n",
      "40 | 5.866628323629062\n",
      "41 | 5.73834202477558\n",
      "42 | 5.60931154412146\n",
      "43 | 5.479692432376578\n",
      "44 | 5.349648550249052\n",
      "45 | 5.21935111306806\n",
      "46 | 5.088977639020193\n",
      "47 | 4.95871080943342\n",
      "48 | 4.8287372516124165\n",
      "49 | 4.699246256698354\n",
      "50 | 4.570428446825663\n",
      "51 | 4.442474407417468\n",
      "52 | 4.315573301739329\n",
      "53 | 4.1899114857666655\n",
      "54 | 4.06567114197167\n",
      "55 | 3.9430289507698304\n",
      "56 | 3.822154818065642\n",
      "57 | 3.7032106765998103\n",
      "58 | 3.5863493776353472\n",
      "59 | 3.4717136879574433\n",
      "60 | 3.3594354052395237\n",
      "61 | 3.2496346026023266\n",
      "62 | 3.1424190107263934\n",
      "63 | 3.037883543245239\n",
      "64 | 2.936109968424001\n",
      "65 | 2.837166727395503\n",
      "66 | 2.7411088965616357\n",
      "67 | 2.647978289245404\n",
      "68 | 2.5578036893646203\n",
      "69 | 2.470601207847691\n",
      "70 | 2.386374750771744\n",
      "71 | 2.3051165868025225\n",
      "72 | 2.2268080004765625\n",
      "73 | 2.1514200171911657\n",
      "74 | 2.078914185452474\n",
      "75 | 2.0092434019568413\n",
      "76 | 1.942352765418761\n",
      "77 | 1.878180445674435\n",
      "78 | 1.8166585554423682\n",
      "79 | 1.757714013167358\n",
      "80 | 1.701269386564977\n",
      "81 | 1.6472437077753648\n",
      "82 | 1.5955532523838325\n",
      "83 | 1.5461122759308468\n",
      "84 | 1.4988337028799135\n",
      "85 | 1.4536297643073632\n",
      "86 | 1.4104125817976414\n",
      "87 | 1.3690946961515975\n",
      "88 | 1.3295895405287124\n",
      "89 | 1.2918118585381402\n",
      "90 | 1.2556780685631834\n",
      "91 | 1.2211065762485298\n",
      "92 | 1.1880180376027898\n",
      "93 | 1.1563355755758589\n",
      "94 | 1.1259849532697557\n",
      "95 | 1.0968947071423207\n",
      "96 | 1.0689962436759246\n",
      "97 | 1.0422239030196307\n",
      "98 | 1.0165149930840172\n",
      "99 | 0.9918097974838884\n",
      "100 | 0.9680515605965179\n",
      "101 | 0.945186452840772\n",
      "102 | 0.9231635190947493\n",
      "103 | 0.9019346129643705\n",
      "104 | 0.8814543193994709\n",
      "105 | 0.8616798679330154\n",
      "106 | 0.8425710385987216\n",
      "107 | 0.8240900623659264\n",
      "108 | 0.8062015177217152\n",
      "109 | 0.7888722248313991\n",
      "110 | 0.7720711385211708\n",
      "111 | 0.7557692411524811\n",
      "112 | 0.7399394362971023\n",
      "113 | 0.7245564439750309\n",
      "114 | 0.7095966980848505\n",
      "115 | 0.6950382465368646\n",
      "116 | 0.6808606544935302\n",
      "117 | 0.6670449110281548\n",
      "118 | 0.6535733394309459\n",
      "119 | 0.6404295113207532\n",
      "120 | 0.6275981646596982\n",
      "121 | 0.6150651257161935\n",
      "122 | 0.6028172349779877\n",
      "123 | 0.5908422769808549\n",
      "124 | 0.579128913988534\n",
      "125 | 0.5676666234356945\n",
      "126 | 0.5564456390267534\n",
      "127 | 0.5454568953687701\n",
      "128 | 0.5346919760060296\n",
      "129 | 0.524143064716259\n",
      "130 | 0.5138028999237985\n",
      "131 | 0.503664732082543\n",
      "132 | 0.49372228388088246\n",
      "133 | 0.4839697131219871\n",
      "134 | 0.4744015781350257\n",
      "135 | 0.46501280557620905\n",
      "136 | 0.45579866048265244\n",
      "137 | 0.4467547184467688\n",
      "138 | 0.4378768397839082\n",
      "139 | 0.4291611455714727\n",
      "140 | 0.4206039954432016\n",
      "141 | 0.4122019670280151\n",
      "142 | 0.403951836928464\n",
      "143 | 0.39585056313935235\n",
      "144 | 0.38789526881262093\n",
      "145 | 0.38008322727990324\n",
      "146 | 0.3724118482492366\n",
      "147 | 0.36487866509745237\n",
      "148 | 0.35748132318441134\n",
      "149 | 0.35021756911976815\n",
      "150 | 0.34308524091727877\n",
      "151 | 0.33608225897563787\n",
      "152 | 0.32920661782869476\n",
      "153 | 0.32245637861151566\n",
      "154 | 0.31582966219208297\n",
      "155 | 0.3093246429216907\n",
      "156 | 0.30293954296002834\n",
      "157 | 0.2966726271337298\n",
      "158 | 0.2905221982898522\n",
      "159 | 0.28448659310813684\n",
      "160 | 0.27856417833823494\n",
      "161 | 0.2727533474302162\n",
      "162 | 0.2670525175287149\n",
      "163 | 0.2614601268029073\n",
      "164 | 0.25597463208631815\n",
      "165 | 0.2505945068020754\n",
      "166 | 0.24531823915077908\n",
      "167 | 0.2401443305396497\n",
      "168 | 0.23507129423290118\n",
      "169 | 0.23009765420461023\n",
      "170 | 0.22522194417656885\n",
      "171 | 0.22044270682467515\n",
      "172 | 0.21575849313857018\n",
      "173 | 0.21116786192011186\n",
      "174 | 0.20666937940732913\n",
      "175 | 0.20226161901133433\n",
      "176 | 0.19794316115450306\n",
      "177 | 0.19371259319906445\n",
      "178 | 0.189568509455943\n",
      "179 | 0.18550951126443935\n",
      "180 | 0.18153420713400434\n",
      "181 | 0.17764121293995988\n",
      "182 | 0.17382915216564043\n",
      "183 | 0.1700966561840154\n",
      "184 | 0.16644236457235256\n",
      "185 | 0.1628649254539874\n",
      "186 | 0.15936299586179165\n",
      "187 | 0.1559352421183086\n",
      "188 | 0.1525803402280284\n",
      "189 | 0.1492969762776147\n",
      "190 | 0.14608384684030673\n",
      "191 | 0.1429396593810795\n",
      "192 | 0.13986313265946285\n",
      "193 | 0.13685299712726404\n",
      "194 | 0.1339079953186901\n",
      "195 | 0.1310268822306945\n",
      "196 | 0.12820842569160978\n",
      "197 | 0.12545140671638225\n",
      "198 | 0.12275461984690783\n",
      "199 | 0.12011687347626541\n",
      "200 | 0.11753699015576201\n",
      "201 | 0.11501380688392873\n",
      "202 | 0.11254617537674805\n",
      "203 | 0.11013296231860364\n",
      "204 | 0.1077730495935042\n",
      "205 | 0.10546533449634353\n",
      "206 | 0.103208729924031\n",
      "207 | 0.10100216454646517\n",
      "208 | 0.09884458295739726\n",
      "209 | 0.09673494580534385\n",
      "210 | 0.09467222990478633\n",
      "211 | 0.0926554283279533\n",
      "212 | 0.09068355047755422\n",
      "213 | 0.08875562214091512\n",
      "214 | 0.08687068552595265\n",
      "215 | 0.08502779927955838\n",
      "216 | 0.08322603848890649\n",
      "217 | 0.08146449466630046\n",
      "218 | 0.07974227571817645\n",
      "219 | 0.0780585058988767\n",
      "220 | 0.0764123257498842\n",
      "221 | 0.07480289202514001\n",
      "222 | 0.07322937760315747\n",
      "223 | 0.07169097138658756\n",
      "224 | 0.07018687818992136\n",
      "225 | 0.06871631861600887\n",
      "226 | 0.06727852892207051\n",
      "227 | 0.06587276087585893\n",
      "228 | 0.06449828160264595\n",
      "229 | 0.06315437342365994\n",
      "230 | 0.061840333686626905\n",
      "231 | 0.06055547458902218\n",
      "232 | 0.05929912299465016\n",
      "233 | 0.05807062024412747\n",
      "234 | 0.056869321959850325\n",
      "235 | 0.0556945978459913\n",
      "236 | 0.05454583148406441\n",
      "237 | 0.053422420124569575\n",
      "238 | 0.052323774475211325\n",
      "239 | 0.05124931848616415\n",
      "240 | 0.05019848913284649\n",
      "241 | 0.049170736196621066\n",
      "242 | 0.04816552204384918\n",
      "243 | 0.04718232140369188\n",
      "244 | 0.046220621145017786\n",
      "245 | 0.04527992005279105\n",
      "246 | 0.044359728604246335\n",
      "247 | 0.043459568745200114\n",
      "248 | 0.04257897366676529\n",
      "249 | 0.04171748758276395\n",
      "250 | 0.04087466550808819\n",
      "251 | 0.040050073038262246\n",
      "252 | 0.03924328613042848\n",
      "253 | 0.03845389088596023\n",
      "254 | 0.03768148333490656\n",
      "255 | 0.0369256692224464\n",
      "256 | 0.03618606379750576\n",
      "257 | 0.03546229160371377\n",
      "258 | 0.03475398627279856\n",
      "259 | 0.034060790320587016\n",
      "260 | 0.0333823549456934\n",
      "261 | 0.03271833983100537\n",
      "262 | 0.032068412948067594\n",
      "263 | 0.03143225036442294\n",
      "264 | 0.030809536053995792\n",
      "265 | 0.030199961710574458\n",
      "266 | 0.02960322656443463\n",
      "267 | 0.029019037202160973\n",
      "268 | 0.028447107389683774\n",
      "269 | 0.027887157898574523\n",
      "270 | 0.02733891633560714\n",
      "271 | 0.026802116975606076\n",
      "272 | 0.026276500597582034\n",
      "273 | 0.02576181432416402\n",
      "274 | 0.025257811464314845\n",
      "275 | 0.024764251359329595\n",
      "276 | 0.024280899232096077\n",
      "277 | 0.02380752603960303\n",
      "278 | 0.023343908328675313\n",
      "279 | 0.02288982809490851\n",
      "280 | 0.022445072644771438\n",
      "281 | 0.02200943446085135\n",
      "282 | 0.021582711070201547\n",
      "283 | 0.02116470491576155\n",
      "284 | 0.02075522323079469\n",
      "285 | 0.02035407791632671\n",
      "286 | 0.01996108542151772\n",
      "287 | 0.01957606662693913\n",
      "288 | 0.019198846730701782\n",
      "289 | 0.018829255137393683\n",
      "290 | 0.018467125349772157\n",
      "291 | 0.018112294863168583\n",
      "292 | 0.017764605062550336\n",
      "293 | 0.017423901122199034\n",
      "294 | 0.017090031907934275\n",
      "295 | 0.016762849881860758\n",
      "296 | 0.016442211009561763\n",
      "297 | 0.016127974669703286\n",
      "298 | 0.015820003565991617\n",
      "299 | 0.015518163641429935\n"
     ]
    }
   ],
   "source": [
    "#use your function with 3 latent features, lr of 0.005 and 300 iterations\n",
    "user_mat, movie_mat = FunkSVD(ratings_mat=ratings_mat,\n",
    "                              latent_features=3,\n",
    "                              learning_rate=0.005,\n",
    "                              iters=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 7.1525123  10.00047635  7.99950524]\n",
      " [ 5.99895315  9.99939558  7.00129872]\n",
      " [ 8.00102569  9.00033239  7.99898774]\n",
      " [ 8.00659736 10.00259352  9.99298426]\n",
      " [ 9.00346307  9.00127494  8.99627831]\n",
      " [ 7.99033977  8.99603392  9.01028025]]\n",
      "[[ 7. 10.  8.]\n",
      " [ 6. 10.  7.]\n",
      " [ 8.  9.  8.]\n",
      " [ 8. 10. 10.]\n",
      " [ 9.  9.  9.]\n",
      " [ 8.  9.  9.]]\n"
     ]
    }
   ],
   "source": [
    "print(np.dot(user_mat, movie_mat))\n",
    "print(ratings_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_**Task: compare the predicted vs actual ratings and write your findings here**_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last time we placed an **np.nan** value into this matrix the entire svd algorithm in python broke.  Let's see if that is still the case using your FunkSVD function.  In the below cell, I have placed a nan into the first cell of your numpy array.  \n",
    "\n",
    "`4.` Use 3 latent features, a learning rate of 0.005, and 450 iterations.  Are you able to run your SVD without it breaking (something that was not true about the python built in)?  Do you get a prediction for the nan value?  What is your prediction for the missing value? Use the cells below to answer these questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[nan, 10.,  8.],\n",
       "        [ 6., 10.,  7.],\n",
       "        [ 8.,  9.,  8.],\n",
       "        [ 8., 10., 10.],\n",
       "        [ 9.,  9.,  9.],\n",
       "        [ 8.,  9.,  9.]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_mat[0, 0] = np.nan\n",
    "ratings_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizaiton Statistics\n",
      "Iterations | Mean Squared Error \n",
      "0 | 1064.6388613732595\n",
      "1 | 858.9601176712201\n",
      "2 | 594.4411081927411\n",
      "3 | 332.2771164712286\n",
      "4 | 148.1164040030606\n",
      "5 | 57.49402053656463\n",
      "6 | 24.012784814432436\n",
      "7 | 13.42717855371512\n",
      "8 | 10.086538796109158\n",
      "9 | 8.88122083074736\n",
      "10 | 8.34531819136841\n",
      "11 | 8.050495216744338\n",
      "12 | 7.855972103494983\n",
      "13 | 7.707784731969549\n",
      "14 | 7.582322504341723\n",
      "15 | 7.468273861819386\n",
      "16 | 7.359895822967392\n",
      "17 | 7.254158146703718\n",
      "18 | 7.149418382790521\n",
      "19 | 7.044770400376678\n",
      "20 | 6.939711020639231\n",
      "21 | 6.833964219388979\n",
      "22 | 6.727386143265955\n",
      "23 | 6.619912734783831\n",
      "24 | 6.511530339151776\n",
      "25 | 6.4022589368933485\n",
      "26 | 6.292142410898844\n",
      "27 | 6.181242769376034\n",
      "28 | 6.069636602019093\n",
      "29 | 5.957412792884861\n",
      "30 | 5.8446709310086\n",
      "31 | 5.731520096721857\n",
      "32 | 5.618077837685297\n",
      "33 | 5.5044692276252185\n",
      "34 | 5.3908259471258475\n",
      "35 | 5.277285353412771\n",
      "36 | 5.163989522759522\n",
      "37 | 5.051084259489859\n",
      "38 | 4.938718072137486\n",
      "39 | 4.827041121658619\n",
      "40 | 4.716204149526087\n",
      "41 | 4.606357395549101\n",
      "42 | 4.497649516630141\n",
      "43 | 4.39022651853869\n",
      "44 | 4.284230713230776\n",
      "45 | 4.179799714320437\n",
      "46 | 4.077065483043411\n",
      "47 | 3.9761534364686844\n",
      "48 | 3.8771816288364063\n",
      "49 | 3.7802600157583934\n",
      "50 | 3.685489809646209\n",
      "51 | 3.5929629331718482\n",
      "52 | 3.5027615758615465\n",
      "53 | 3.4149578571234462\n",
      "54 | 3.3296135971666674\n",
      "55 | 3.2467801954311843\n",
      "56 | 3.1664986143668914\n",
      "57 | 3.0887994647200925\n",
      "58 | 3.013703186948889\n",
      "59 | 2.941220322029065\n",
      "60 | 2.871351863757763\n",
      "61 | 2.804089683731293\n",
      "62 | 2.739417019479694\n",
      "63 | 2.6773090157848554\n",
      "64 | 2.6177333089887744\n",
      "65 | 2.5606506441018766\n",
      "66 | 2.506015514731582\n",
      "67 | 2.4537768162459153\n",
      "68 | 2.4038785031414958\n",
      "69 | 2.356260242271147\n",
      "70 | 2.3108580543744854\n",
      "71 | 2.267604937217135\n",
      "72 | 2.226431464551575\n",
      "73 | 2.187266356039508\n",
      "74 | 2.1500370141977365\n",
      "75 | 2.114670025326961\n",
      "76 | 2.0810916222362694\n",
      "77 | 2.0492281073738465\n",
      "78 | 2.019006235702744\n",
      "79 | 1.9903535573146536\n",
      "80 | 1.9631987203477355\n",
      "81 | 1.93747173526651\n",
      "82 | 1.913104201972242\n",
      "83 | 1.8900295015435948\n",
      "84 | 1.8681829546648225\n",
      "85 | 1.8475019489862199\n",
      "86 | 1.8279260377864632\n",
      "87 | 1.8093970123751077\n",
      "88 | 1.7918589506926081\n",
      "89 | 1.7752582445429812\n",
      "90 | 1.7595436078356275\n",
      "91 | 1.744666068126973\n",
      "92 | 1.7305789436429648\n",
      "93 | 1.7172378078377306\n",
      "94 | 1.7046004434058377\n",
      "95 | 1.6926267875200698\n",
      "96 | 1.6812788699177583\n",
      "97 | 1.6705207453091033\n",
      "98 | 1.6603184214336835\n",
      "99 | 1.6506397839483824\n",
      "100 | 1.6414545191932306\n",
      "101 | 1.6327340357518394\n",
      "102 | 1.6244513856019134\n",
      "103 | 1.6165811855387027\n",
      "104 | 1.6090995394503496\n",
      "105 | 1.6019839619301857\n",
      "106 | 1.595213303625047\n",
      "107 | 1.588767678642361\n",
      "108 | 1.582628394270372\n",
      "109 | 1.5767778832056452\n",
      "110 | 1.5711996384289004\n",
      "111 | 1.5658781508247448\n",
      "112 | 1.560798849600824\n",
      "113 | 1.5559480455284789\n",
      "114 | 1.5513128769983655\n",
      "115 | 1.5468812588606773\n",
      "116 | 1.542641834000257\n",
      "117 | 1.5385839275806101\n",
      "118 | 1.5346975038788784\n",
      "119 | 1.5309731256236598\n",
      "120 | 1.5274019157405665\n",
      "121 | 1.5239755214054314\n",
      "122 | 1.5206860803018136\n",
      "123 | 1.517526188977636\n",
      "124 | 1.5144888731957227\n",
      "125 | 1.5115675601734277\n",
      "126 | 1.5087560526083048\n",
      "127 | 1.5060485043889746\n",
      "128 | 1.5034393978932885\n",
      "129 | 1.5009235227790203\n",
      "130 | 1.498495956176109\n",
      "131 | 1.4961520441930876\n",
      "132 | 1.4938873846545402\n",
      "133 | 1.4916978109902204\n",
      "134 | 1.4895793772007369\n",
      "135 | 1.4875283438286369\n",
      "136 | 1.4855411648677637\n",
      "137 | 1.4836144755476683\n",
      "138 | 1.4817450809334394\n",
      "139 | 1.4799299452854142\n",
      "140 | 1.4781661821261305\n",
      "141 | 1.4764510449658608\n",
      "142 | 1.474781918640683\n",
      "143 | 1.4731563112206203\n",
      "144 | 1.4715718464477952\n",
      "145 | 1.470026256667575\n",
      "146 | 1.468517376218096\n",
      "147 | 1.4670431352459672\n",
      "148 | 1.4656015539183151\n",
      "149 | 1.4641907370032752\n",
      "150 | 1.4628088687931555\n",
      "151 | 1.4614542083463187\n",
      "152 | 1.4601250850254928\n",
      "153 | 1.4588198943118347\n",
      "154 | 1.457537093875672\n",
      "155 | 1.4562751998860755\n",
      "156 | 1.4550327835428685\n",
      "157 | 1.4538084678156884\n",
      "158 | 1.4526009243760365\n",
      "159 | 1.451408870709148\n",
      "160 | 1.4502310673934513\n",
      "161 | 1.449066315536425\n",
      "162 | 1.4479134543562693\n",
      "163 | 1.4467713588997517\n",
      "164 | 1.4456389378872085\n",
      "165 | 1.4445151316762939\n",
      "166 | 1.4433989103367928\n",
      "167 | 1.4422892718291749\n",
      "168 | 1.441185240280394\n",
      "169 | 1.4400858643505037\n",
      "170 | 1.4389902156845107\n",
      "171 | 1.437897387443995\n",
      "172 | 1.4368064929135405\n",
      "173 | 1.4357166641774093\n",
      "174 | 1.43462705086203\n",
      "175 | 1.4335368189404856\n",
      "176 | 1.4324451495950437\n",
      "177 | 1.4313512381344993\n",
      "178 | 1.430254292962933\n",
      "179 | 1.429153534596995\n",
      "180 | 1.4280481947288257\n",
      "181 | 1.4269375153321344\n",
      "182 | 1.4258207478088987\n",
      "183 | 1.4246971521744805\n",
      "184 | 1.4235659962791085\n",
      "185 | 1.422426555063658\n",
      "186 | 1.4212781098480518\n",
      "187 | 1.4201199476504478\n",
      "188 | 1.4189513605357735\n",
      "189 | 1.41777164499203\n",
      "190 | 1.4165801013330939\n",
      "191 | 1.4153760331267007\n",
      "192 | 1.4141587466464476\n",
      "193 | 1.4129275503467673\n",
      "194 | 1.411681754359825\n",
      "195 | 1.4104206700133768\n",
      "196 | 1.4091436093688787\n",
      "197 | 1.4078498847788685\n",
      "198 | 1.406538808463011\n",
      "199 | 1.4052096921020933\n",
      "200 | 1.4038618464494506\n",
      "201 | 1.4024945809590925\n",
      "202 | 1.4011072034302106\n",
      "203 | 1.3996990196675234\n",
      "204 | 1.3982693331570555\n",
      "205 | 1.3968174447570123\n",
      "206 | 1.3953426524034513\n",
      "207 | 1.3938442508303985\n",
      "208 | 1.3923215313043007\n",
      "209 | 1.3907737813724868\n",
      "210 | 1.3892002846255482\n",
      "211 | 1.387600320473541\n",
      "212 | 1.3859731639358313\n",
      "213 | 1.384318085444608\n",
      "214 | 1.3826343506620118\n",
      "215 | 1.3809212203108585\n",
      "216 | 1.3791779500190584\n",
      "217 | 1.3774037901777216\n",
      "218 | 1.375597985813101\n",
      "219 | 1.373759776472502\n",
      "220 | 1.3718883961242192\n",
      "221 | 1.3699830730718106\n",
      "222 | 1.3680430298827995\n",
      "223 | 1.3660674833320803\n",
      "224 | 1.36405564436028\n",
      "225 | 1.3620067180473336\n",
      "226 | 1.3599199036015643\n",
      "227 | 1.3577943943646213\n",
      "228 | 1.355629377832578\n",
      "229 | 1.3534240356935616\n",
      "230 | 1.3511775438823332\n",
      "231 | 1.348889072652123\n",
      "232 | 1.3465577866642626\n",
      "233 | 1.3441828450959625\n",
      "234 | 1.3417634017667062\n",
      "235 | 1.3392986052837634\n",
      "236 | 1.3367875992072291\n",
      "237 | 1.3342295222352005\n",
      "238 | 1.3316235084094943\n",
      "239 | 1.3289686873424695\n",
      "240 | 1.3262641844655165\n",
      "241 | 1.3235091212996883\n",
      "242 | 1.3207026157490758\n",
      "243 | 1.31784378241747\n",
      "244 | 1.3149317329488406\n",
      "245 | 1.3119655763922553\n",
      "246 | 1.3089444195917388\n",
      "247 | 1.305867367601658\n",
      "248 | 1.3027335241282154\n",
      "249 | 1.299541991997587\n",
      "250 | 1.2962918736512508\n",
      "251 | 1.2929822716690709\n",
      "252 | 1.2896122893205988\n",
      "253 | 1.2861810311452238\n",
      "254 | 1.2826876035615273\n",
      "255 | 1.2791311155064637\n",
      "256 | 1.2755106791047022\n",
      "257 | 1.2718254103686104\n",
      "258 | 1.2680744299293099\n",
      "259 | 1.2642568637991052\n",
      "260 | 1.2603718441656728\n",
      "261 | 1.2564185102182654\n",
      "262 | 1.2523960090062174\n",
      "263 | 1.2483034963299067\n",
      "264 | 1.2441401376643582\n",
      "265 | 1.2399051091155988\n",
      "266 | 1.2355975984097127\n",
      "267 | 1.2312168059147024\n",
      "268 | 1.226761945694929\n",
      "269 | 1.2222322465980213\n",
      "270 | 1.2176269533740358\n",
      "271 | 1.2129453278264415\n",
      "272 | 1.2081866499946199\n",
      "273 | 1.2033502193672803\n",
      "274 | 1.198435356126237\n",
      "275 | 1.193441402419799\n",
      "276 | 1.1883677236650105\n",
      "277 | 1.1832137098777495\n",
      "278 | 1.1779787770296901\n",
      "279 | 1.1726623684309296\n",
      "280 | 1.167263956137041\n",
      "281 | 1.1617830423790498\n",
      "282 | 1.1562191610148962\n",
      "283 | 1.1505718790005885\n",
      "284 | 1.1448407978792776\n",
      "285 | 1.139025555286266\n",
      "286 | 1.1331258264678465\n",
      "287 | 1.1271413258116383\n",
      "288 | 1.1210718083861253\n",
      "289 | 1.11491707148664\n",
      "290 | 1.1086769561852856\n",
      "291 | 1.1023513488816588\n",
      "292 | 1.095940182851582\n",
      "293 | 1.0894434397904678\n",
      "294 | 1.0828611513480322\n",
      "295 | 1.0761934006508718\n",
      "296 | 1.069440323809208\n",
      "297 | 1.0626021114040503\n",
      "298 | 1.0556790099507871\n",
      "299 | 1.04867132333518\n",
      "300 | 1.0415794142174932\n",
      "301 | 1.0344037054004993\n",
      "302 | 1.027144681156885\n",
      "303 | 1.019802888511434\n",
      "304 | 1.012378938473512\n",
      "305 | 1.0048735072149222\n",
      "306 | 0.9972873371884825\n",
      "307 | 0.9896212381823514\n",
      "308 | 0.9818760883052704\n",
      "309 | 0.9740528348977504\n",
      "310 | 0.966152495364212\n",
      "311 | 0.9581761579211968\n",
      "312 | 0.9501249822567002\n",
      "313 | 0.9420002000956923\n",
      "314 | 0.9338031156670926\n",
      "315 | 0.925535106067373\n",
      "316 | 0.9171976215162075\n",
      "317 | 0.90879218549964\n",
      "318 | 0.9003203947964377\n",
      "319 | 0.8917839193834135\n",
      "320 | 0.8831845022157723\n",
      "321 | 0.8745239588787047\n",
      "322 | 0.8658041771067274\n",
      "323 | 0.8570271161675229\n",
      "324 | 0.8481948061073659\n",
      "325 | 0.8393093468554955\n",
      "326 | 0.8303729071851662\n",
      "327 | 0.8213877235295194\n",
      "328 | 0.812356098650676\n",
      "329 | 0.8032804001610776\n",
      "330 | 0.7941630588963082\n",
      "331 | 0.7850065671392485\n",
      "332 | 0.775813476695819\n",
      "333 | 0.7665863968230653\n",
      "334 | 0.7573279920108272\n",
      "335 | 0.7480409796187588\n",
      "336 | 0.738728127371047\n",
      "337 | 0.7293922507115854\n",
      "338 | 0.7200362100230031\n",
      "339 | 0.7106629077134717\n",
      "340 | 0.7012752851757075\n",
      "341 | 0.6918763196232007\n",
      "342 | 0.6824690208091242\n",
      "343 | 0.6730564276340586\n",
      "344 | 0.6636416046490401\n",
      "345 | 0.6542276384609914\n",
      "346 | 0.6448176340481113\n",
      "347 | 0.6354147109932062\n",
      "348 | 0.6260219996434075\n",
      "349 | 0.6166426372051463\n",
      "350 | 0.6072797637835868\n",
      "351 | 0.5979365183761655\n",
      "352 | 0.5886160348300497\n",
      "353 | 0.5793214377737843\n",
      "354 | 0.5700558385334817\n",
      "355 | 0.560822331044187\n",
      "356 | 0.5516239877671676\n",
      "357 | 0.5424638556240585\n",
      "358 | 0.5333449519586899\n",
      "359 | 0.5242702605376569\n",
      "360 | 0.5152427276004693\n",
      "361 | 0.5062652579701248\n",
      "362 | 0.4973407112348298\n",
      "363 | 0.4884718980113536\n",
      "364 | 0.4796615763002961\n",
      "365 | 0.47091244794331727\n",
      "366 | 0.4622271551919721\n",
      "367 | 0.45360827739747794\n",
      "368 | 0.44505832783035487\n",
      "369 | 0.4365797506383449\n",
      "370 | 0.4281749179506012\n",
      "371 | 0.41984612713559083\n",
      "372 | 0.4115955982195844\n",
      "373 | 0.40342547147200314\n",
      "374 | 0.3953378051633437\n",
      "375 | 0.3873345735006532\n",
      "376 | 0.3794176647449916\n",
      "377 | 0.37158887951454833\n",
      "378 | 0.36384992927641213\n",
      "379 | 0.35620243502937504\n",
      "380 | 0.3486479261793148\n",
      "381 | 0.3411878396081308\n",
      "382 | 0.3338235189364343\n",
      "383 | 0.32655621397952617\n",
      "384 | 0.31938708039555236\n",
      "385 | 0.31231717952395777\n",
      "386 | 0.3053474784118798\n",
      "387 | 0.2984788500253418\n",
      "388 | 0.29171207364158863\n",
      "389 | 0.28504783541831835\n",
      "390 | 0.27848672913502615\n",
      "391 | 0.2720292571011454\n",
      "392 | 0.2656758312252171\n",
      "393 | 0.25942677423887395\n",
      "394 | 0.25328232106898485\n",
      "395 | 0.2472426203510412\n",
      "396 | 0.2413077360764007\n",
      "397 | 0.2354776493658427\n",
      "398 | 0.22975226036158505\n",
      "399 | 0.22413139022973716\n",
      "400 | 0.21861478326494746\n",
      "401 | 0.21320210908899984\n",
      "402 | 0.20789296493488763\n",
      "403 | 0.20268687800796512\n",
      "404 | 0.1975833079157239\n",
      "405 | 0.192581649157777\n",
      "406 | 0.1876812336677026\n",
      "407 | 0.1828813333985283\n",
      "408 | 0.17818116294370145\n",
      "409 | 0.17357988218565862\n",
      "410 | 0.1690765989641672\n",
      "411 | 0.1646703717569684\n",
      "412 | 0.16036021236535464\n",
      "413 | 0.15614508859768522\n",
      "414 | 0.152023926944056\n",
      "415 | 0.14799561523565918\n",
      "416 | 0.14405900528267582\n",
      "417 | 0.14021291548485837\n",
      "418 | 0.1364561334093234\n",
      "419 | 0.13278741833034616\n",
      "420 | 0.12920550372639314\n",
      "421 | 0.12570909972987077\n",
      "422 | 0.1222968955255089\n",
      "423 | 0.11896756169358616\n",
      "424 | 0.11571975249459301\n",
      "425 | 0.11255210809224653\n",
      "426 | 0.10946325671212268\n",
      "427 | 0.10645181673351027\n",
      "428 | 0.10351639871241884\n",
      "429 | 0.10065560733394462\n",
      "430 | 0.097868043292608\n",
      "431 | 0.09515230509943752\n",
      "432 | 0.09250699081499135\n",
      "433 | 0.08993069970767903\n",
      "434 | 0.08742203383706985\n",
      "435 | 0.08497959956209683\n",
      "436 | 0.0826020089742756\n",
      "437 | 0.080287881256344\n",
      "438 | 0.07803584396685057\n",
      "439 | 0.07584453425147002\n",
      "440 | 0.07371259998198809\n",
      "441 | 0.07163870082403125\n",
      "442 | 0.06962150923481673\n",
      "443 | 0.0676597113922838\n",
      "444 | 0.06575200805712743\n",
      "445 | 0.06389711536934385\n",
      "446 | 0.06209376558099264\n",
      "447 | 0.060340707726982676\n",
      "448 | 0.058636708235747854\n",
      "449 | 0.05698055148171816\n"
     ]
    }
   ],
   "source": [
    "# run SVD on the matrix with the missing value\n",
    "# use your function with 3 latent features, lr of 0.005 and 450 iterations\n",
    "user_mat, movie_mat = FunkSVD(ratings_mat=ratings_mat,\n",
    "                              latent_features=3,\n",
    "                              learning_rate=0.005,\n",
    "                              iters=450)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted value for the missing rating is: 6.799278015389177\n",
      "\n",
      "That's right! You just predicted a rating for a user-movie pair that was never rated!\n",
      "But if you look in the original matrix, this was actually a value of 7. Not bad!\n"
     ]
    }
   ],
   "source": [
    "preds = np.dot(user_mat, movie_mat)\n",
    "print(\"The predicted value for the missing rating is: {}\".format(preds[0,0]))\n",
    "print()\n",
    "print(\"That's right! You just predicted a rating for a user-movie pair that was never rated!\")\n",
    "print(\"But if you look in the original matrix, this was actually a value of 7. Not bad!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 6.79927802  9.99644121  8.00370784]\n",
      " [ 5.99733358  9.99990057  7.00172179]\n",
      " [ 7.89279109  8.95792566  8.11035275]\n",
      " [ 8.09221844 10.03864715  9.90468108]\n",
      " [ 9.02236761  9.00802682  8.97747777]\n",
      " [ 7.9786558   8.99231456  9.02113112]]\n",
      "[[nan 10.  8.]\n",
      " [ 6. 10.  7.]\n",
      " [ 8.  9.  8.]\n",
      " [ 8. 10. 10.]\n",
      " [ 9.  9.  9.]\n",
      " [ 8.  9.  9.]]\n"
     ]
    }
   ],
   "source": [
    "print(np.dot(user_mat, movie_mat))\n",
    "print(ratings_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's extend this to a more realistic example. Unfortunately, running this function on your entire user-movie matrix is still not something you likely want to do on your local machine.  However, we can see how well this example extends to 1000 users.  In the above portion, you were using a very small subset of data with no missing values.\n",
    "\n",
    "`5.` Given the size of this matrix, this will take quite a bit of time.  Consider the following hyperparameters: 3 latent features, 0.005 learning rate, and 500 iterations.  Grab a snack, take a walk, and this should be done running in a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizaiton Statistics\n",
      "Iterations | Mean Squared Error \n",
      "0 | 1111.888006569856\n",
      "1 | 952.9657136762671\n",
      "2 | 722.7373740334392\n",
      "3 | 456.36587170541975\n",
      "4 | 231.234177904896\n",
      "5 | 97.63151055446653\n",
      "6 | 39.89810943779286\n",
      "7 | 19.621372381221477\n",
      "8 | 12.911183881218799\n",
      "9 | 10.516157162988222\n",
      "10 | 9.521432866129274\n",
      "11 | 9.037051267246234\n",
      "12 | 8.766768933284743\n",
      "13 | 8.597081223781716\n",
      "14 | 8.4782705247685\n",
      "15 | 8.386305220513377\n",
      "16 | 8.308779724489284\n",
      "17 | 8.238995183522999\n",
      "18 | 8.173210108014203\n",
      "19 | 8.109274836977885\n",
      "20 | 8.045921203847286\n",
      "21 | 7.982380357991922\n",
      "22 | 7.918171780680401\n",
      "23 | 7.852984449118896\n",
      "24 | 7.786608848140698\n",
      "25 | 7.718897605094741\n",
      "26 | 7.649742497918329\n",
      "27 | 7.579060951326545\n",
      "28 | 7.506788090217061\n",
      "29 | 7.432872077561797\n",
      "30 | 7.357271409165852\n",
      "31 | 7.279953382875134\n",
      "32 | 7.20089327726076\n",
      "33 | 7.120073960972596\n",
      "34 | 7.037485763734212\n",
      "35 | 6.953126504971337\n",
      "36 | 6.867001614715778\n",
      "37 | 6.779124304478444\n",
      "38 | 6.689515759584451\n",
      "39 | 6.598205332769651\n",
      "40 | 6.505230723884514\n",
      "41 | 6.410638133675652\n",
      "42 | 6.3144823816182845\n",
      "43 | 6.216826979142404\n",
      "44 | 6.117744150625803\n",
      "45 | 6.017314795402559\n",
      "46 | 5.915628384867087\n",
      "47 | 5.8127827896129745\n",
      "48 | 5.708884032472875\n",
      "49 | 5.604045964344339\n",
      "50 | 5.498389860806565\n",
      "51 | 5.392043938753104\n",
      "52 | 5.285142793576403\n",
      "53 | 5.177826758826012\n",
      "54 | 5.07024119169841\n",
      "55 | 4.96253568917734\n",
      "56 | 4.854863241093521\n",
      "57 | 4.747379327778807\n",
      "58 | 4.6402409713112505\n",
      "59 | 4.5336057505488485\n",
      "60 | 4.4276307911894826\n",
      "61 | 4.322471742940962\n",
      "62 | 4.218281756503538\n",
      "63 | 4.115210473434527\n",
      "64 | 4.013403042058924\n",
      "65 | 3.912999172402237\n",
      "66 | 3.8141322426470943\n",
      "67 | 3.7169284688621107\n",
      "68 | 3.6215061487333347\n",
      "69 | 3.5279749887718106\n",
      "70 | 3.43643552300553\n",
      "71 | 3.346978629531247\n",
      "72 | 3.2596851495446106\n",
      "73 | 3.174625611634604\n",
      "74 | 3.0918600622707997\n",
      "75 | 3.011438001577815\n",
      "76 | 2.9333984217314124\n",
      "77 | 2.857769943665543\n",
      "78 | 2.7845710462918274\n",
      "79 | 2.7138103811319705\n",
      "80 | 2.645487164176234\n",
      "81 | 2.5795916359238387\n",
      "82 | 2.5161055799420224\n",
      "83 | 2.4550028899021155\n",
      "84 | 2.3962501749049863\n",
      "85 | 2.3398073929826757\n",
      "86 | 2.2856285029388586\n",
      "87 | 2.2336621251440563\n",
      "88 | 2.183852202507107\n",
      "89 | 2.136138653572102\n",
      "90 | 2.0904580105119246\n",
      "91 | 2.0467440356758995\n",
      "92 | 2.004928311271897\n",
      "93 | 1.9649407976976268\n",
      "94 | 1.9267103569577202\n",
      "95 | 1.8901652384930743\n",
      "96 | 1.8552335255899752\n",
      "97 | 1.8218435413155336\n",
      "98 | 1.7899242136330824\n",
      "99 | 1.7594053999796528\n",
      "100 | 1.7302181721340872\n",
      "101 | 1.702295062667789\n",
      "102 | 1.6755702746513272\n",
      "103 | 1.6499798565930912\n",
      "104 | 1.6254618448151594\n",
      "105 | 1.6019563756320188\n",
      "106 | 1.5794057697975383\n",
      "107 | 1.5577545917299\n",
      "108 | 1.5369496860223126\n",
      "109 | 1.5169401937044986\n",
      "110 | 1.4976775506450581\n",
      "111 | 1.4791154703821656\n",
      "112 | 1.4612099135485943\n",
      "113 | 1.4439190459189786\n",
      "114 | 1.4272031869607142\n",
      "115 | 1.411024750616469\n",
      "116 | 1.395348179891578\n",
      "117 | 1.3801398766654867\n",
      "118 | 1.3653681279959895\n",
      "119 | 1.3510030300401734\n",
      "120 | 1.3370164105779647\n",
      "121 | 1.3233817509946089\n",
      "122 | 1.3100741084575274\n",
      "123 | 1.2970700389116696\n",
      "124 | 1.2843475214154438\n",
      "125 | 1.271885884247134\n",
      "126 | 1.2596657331285586\n",
      "127 | 1.2476688818386763\n",
      "128 | 1.235878285424353\n",
      "129 | 1.2242779761581135\n",
      "130 | 1.2128530023425144\n",
      "131 | 1.2015893700180489\n",
      "132 | 1.190473987594375\n",
      "133 | 1.1794946133939328\n",
      "134 | 1.1686398060710168\n",
      "135 | 1.1578988778481905\n",
      "136 | 1.1472618504947176\n",
      "137 | 1.1367194139581642\n",
      "138 | 1.1262628875499803\n",
      "139 | 1.1158841835780233\n",
      "140 | 1.1055757733138862\n",
      "141 | 1.0953306551793236\n",
      "142 | 1.0851423250346497\n",
      "143 | 1.0750047484516685\n",
      "144 | 1.0649123348546572\n",
      "145 | 1.0548599134148826\n",
      "146 | 1.0448427105867972\n",
      "147 | 1.034856329177408\n",
      "148 | 1.0248967288439965\n",
      "149 | 1.0149602079195024\n",
      "150 | 1.0050433864693709\n",
      "151 | 0.9951431904878655\n",
      "152 | 0.9852568371467547\n",
      "153 | 0.9753818210136639\n",
      "154 | 0.9655159011621216\n",
      "155 | 0.9556570890997008\n",
      "156 | 0.9458036374451775\n",
      "157 | 0.935954029289921\n",
      "158 | 0.9261069681828203\n",
      "159 | 0.9162613686821726\n",
      "160 | 0.9064163474216961\n",
      "161 | 0.8965712146415651\n",
      "162 | 0.8867254661390327\n",
      "163 | 0.8768787755961727\n",
      "164 | 0.8670309872459914\n",
      "165 | 0.8571821088407213\n",
      "166 | 0.8473323048892912\n",
      "167 | 0.8374818901335451\n",
      "168 | 0.8276313232354187\n",
      "169 | 0.817781200649749\n",
      "170 | 0.8079322506596134\n",
      "171 | 0.7980853275534302\n",
      "172 | 0.7882414059249934\n",
      "173 | 0.778401575079637\n",
      "174 | 0.7685670335315818\n",
      "175 | 0.7587390835792398\n",
      "176 | 0.7489191259469469\n",
      "177 | 0.7391086544831245\n",
      "178 | 0.7293092509063523\n",
      "179 | 0.7195225795923016\n",
      "180 | 0.7097503823956776\n",
      "181 | 0.6999944735026855\n",
      "182 | 0.6902567343105888\n",
      "183 | 0.6805391083321941\n",
      "184 | 0.6708435961238434\n",
      "185 | 0.6611722502368745\n",
      "186 | 0.6515271701929775\n",
      "187 | 0.6419104974850166\n",
      "188 | 0.6323244106055359\n",
      "189 | 0.6227711201059024\n",
      "190 | 0.6132528636897003\n",
      "191 | 0.6037719013445662\n",
      "192 | 0.5943305105172122\n",
      "193 | 0.5849309813368363\n",
      "194 | 0.5755756118925792\n",
      "195 | 0.5662667035709508\n",
      "196 | 0.5570065564595833\n",
      "197 | 0.547797464823799\n",
      "198 | 0.5386417126627383\n",
      "199 | 0.5295415693518962\n",
      "200 | 0.5204992853790305\n",
      "201 | 0.5115170881803704\n",
      "202 | 0.5025971780841129\n",
      "203 | 0.49374172436806557\n",
      "204 | 0.48495286143819805\n",
      "205 | 0.4762326851346698\n",
      "206 | 0.4675832491717404\n",
      "207 | 0.4590065617176655\n",
      "208 | 0.45050458212041056\n",
      "209 | 0.44207921778468473\n",
      "210 | 0.43373232120542266\n",
      "211 | 0.42546568716244565\n",
      "212 | 0.41728105008062577\n",
      "213 | 0.40918008155937363\n",
      "214 | 0.40116438807484317\n",
      "215 | 0.3932355088577543\n",
      "216 | 0.3853949139491548\n",
      "217 | 0.377644002435998\n",
      "218 | 0.36998410086784117\n",
      "219 | 0.36241646185538123\n",
      "220 | 0.3549422628511417\n",
      "221 | 0.347562605111853\n",
      "222 | 0.3402785128417952\n",
      "223 | 0.3330909325155849\n",
      "224 | 0.32600073237855415\n",
      "225 | 0.31900870212222626\n",
      "226 | 0.3121155527319858\n",
      "227 | 0.3053219165034927\n",
      "228 | 0.29862834722398723\n",
      "229 | 0.29203532051412856\n",
      "230 | 0.2855432343256763\n",
      "231 | 0.27915240958984777\n",
      "232 | 0.2728630910109173\n",
      "233 | 0.2666754479992266\n",
      "234 | 0.2605895757374747\n",
      "235 | 0.25460549637394975\n",
      "236 | 0.24872316033606368\n",
      "237 | 0.2429424477573615\n",
      "238 | 0.23726317001104477\n",
      "239 | 0.23168507134284294\n",
      "240 | 0.22620783059601832\n",
      "241 | 0.2208310630211925\n",
      "242 | 0.21555432216362339\n",
      "243 | 0.2103771018205852\n",
      "244 | 0.20529883806148763\n",
      "245 | 0.2003189113034474\n",
      "246 | 0.19543664843506092\n",
      "247 | 0.19065132498127849\n",
      "248 | 0.18596216730233991\n",
      "249 | 0.18136835481996189\n",
      "250 | 0.1768690222640344\n",
      "251 | 0.17246326193338946\n",
      "252 | 0.16815012596431306\n",
      "253 | 0.1639286286007394\n",
      "254 | 0.15979774846033165\n",
      "255 | 0.15575643079083018\n",
      "256 | 0.15180358971140787\n",
      "257 | 0.14793811043393904\n",
      "258 | 0.14415885145947938\n",
      "259 | 0.14046464674542866\n",
      "260 | 0.13685430783924127\n",
      "261 | 0.13332662597475262\n",
      "262 | 0.12988037412755754\n",
      "263 | 0.12651430902609542\n",
      "264 | 0.12322717311548495\n",
      "265 | 0.12001769647132406\n",
      "266 | 0.11688459866105211\n",
      "267 | 0.11382659055071175\n",
      "268 | 0.11084237605520791\n",
      "269 | 0.10793065383044582\n",
      "270 | 0.10509011890599625\n",
      "271 | 0.10231946425715979\n",
      "272 | 0.09961738231556379\n",
      "273 | 0.09698256641763313\n",
      "274 | 0.09441371219053803\n",
      "275 | 0.09190951887536469\n",
      "276 | 0.08946869058752391\n",
      "277 | 0.08708993751454291\n",
      "278 | 0.08477197705159574\n",
      "279 | 0.08251353487527696\n",
      "280 | 0.08031334595626725\n",
      "281 | 0.07817015551171927\n",
      "282 | 0.07608271989825234\n",
      "283 | 0.07404980744664326\n",
      "284 | 0.07207019923934332\n",
      "285 | 0.07014268983208877\n",
      "286 | 0.06826608792093769\n",
      "287 | 0.06643921695614907\n",
      "288 | 0.0646609157043898\n",
      "289 | 0.06293003876080171\n",
      "290 | 0.0612454570125279\n",
      "291 | 0.05960605805532401\n",
      "292 | 0.0580107465648961\n",
      "293 | 0.05645844462468419\n",
      "294 | 0.05494809201175447\n",
      "295 | 0.053478646442538046\n",
      "296 | 0.052049083780117716\n",
      "297 | 0.050658398204776287\n",
      "298 | 0.04930560234951121\n",
      "299 | 0.04798972740221013\n",
      "300 | 0.046709823176150654\n",
      "301 | 0.04546495815049348\n",
      "302 | 0.04425421948237105\n",
      "303 | 0.043076712992191145\n",
      "304 | 0.041931563123707355\n",
      "305 | 0.04081791288038363\n",
      "306 | 0.03973492373955482\n",
      "307 | 0.03868177554583135\n",
      "308 | 0.03765766638515063\n",
      "309 | 0.036661812440860655\n",
      "310 | 0.035693447833138485\n",
      "311 | 0.03475182444304123\n",
      "312 | 0.033836211722416236\n",
      "313 | 0.032945896490852056\n",
      "314 | 0.032080182720819576\n",
      "315 | 0.031238391312089206\n",
      "316 | 0.030419859856482635\n",
      "317 | 0.029623942393943406\n",
      "318 | 0.028850009160904657\n",
      "319 | 0.028097446331832695\n",
      "320 | 0.027365655754848815\n",
      "321 | 0.026654054682208706\n",
      "322 | 0.02596207549646742\n",
      "323 | 0.025289165433007285\n",
      "324 | 0.024634786299674603\n",
      "325 | 0.023998414194148165\n",
      "326 | 0.02337953921966824\n",
      "327 | 0.02277766519969833\n",
      "328 | 0.0221923093920589\n",
      "329 | 0.02162300220304625\n",
      "330 | 0.021069286901997244\n",
      "331 | 0.020530719336741207\n",
      "332 | 0.020006867650350937\n",
      "333 | 0.01949731199956176\n",
      "334 | 0.01900164427520122\n",
      "335 | 0.018519467824964934\n",
      "336 | 0.018050397178809907\n",
      "337 | 0.017594057777244803\n",
      "338 | 0.017150085702756564\n",
      "339 | 0.01671812741458895\n",
      "340 | 0.01629783948707912\n",
      "341 | 0.015888888351712982\n",
      "342 | 0.01549095004307785\n",
      "343 | 0.015103709948829362\n",
      "344 | 0.014726862563809654\n",
      "345 | 0.014360111248409618\n",
      "346 | 0.01400316799127558\n",
      "347 | 0.013655753176422088\n",
      "348 | 0.013317595354824071\n",
      "349 | 0.012988431020524877\n",
      "350 | 0.012668004391305632\n",
      "351 | 0.012356067193935996\n",
      "352 | 0.012052378454020046\n",
      "353 | 0.011756704290450498\n",
      "354 | 0.011468817714453367\n",
      "355 | 0.01118849843322409\n",
      "356 | 0.010915532658133128\n",
      "357 | 0.010649712917465581\n",
      "358 | 0.010390837873677214\n",
      "359 | 0.010138712145113403\n",
      "360 | 0.009893146132160968\n",
      "361 | 0.009653955847769877\n",
      "362 | 0.009420962752307041\n",
      "363 | 0.00919399359267738\n",
      "364 | 0.008972880245647439\n",
      "365 | 0.008757459565321538\n",
      "366 | 0.008547573234694208\n",
      "367 | 0.008343067621214636\n",
      "368 | 0.008143793636292303\n",
      "369 | 0.007949606598671215\n",
      "370 | 0.007760366101603097\n",
      "371 | 0.007575935883740162\n",
      "372 | 0.007396183703675034\n",
      "373 | 0.00722098121805189\n",
      "374 | 0.007050203863169798\n",
      "375 | 0.006883730740006862\n",
      "376 | 0.006721444502579598\n",
      "377 | 0.00656323124956899\n",
      "378 | 0.006408980419132341\n",
      "379 | 0.006258584686824076\n",
      "380 | 0.006111939866552228\n",
      "381 | 0.005968944814492724\n",
      "382 | 0.005829501335887908\n",
      "383 | 0.005693514094655468\n",
      "384 | 0.005560890525733613\n",
      "385 | 0.005431540750091634\n",
      "386 | 0.005305377492334404\n",
      "387 | 0.005182316000830087\n",
      "388 | 0.005062273970291558\n",
      "389 | 0.0049451714667470515\n",
      "390 | 0.004830930854826422\n",
      "391 | 0.004719476727307376\n",
      "392 | 0.004610735836847608\n",
      "393 | 0.004504637029846678\n",
      "394 | 0.0044011111823726605\n",
      "395 | 0.004300091138095414\n",
      "396 | 0.0042015116481654196\n",
      "397 | 0.004105309312980592\n",
      "398 | 0.004011422525788654\n",
      "399 | 0.003919791418063782\n",
      "400 | 0.0038303578066076894\n",
      "401 | 0.003743065142323237\n",
      "402 | 0.00365785846060765\n",
      "403 | 0.003574684333315574\n",
      "404 | 0.00349349082224532\n",
      "405 | 0.0034142274340996136\n",
      "406 | 0.003336845076873082\n",
      "407 | 0.0032612960176283635\n",
      "408 | 0.0031875338416086055\n",
      "409 | 0.0031155134126499085\n",
      "410 | 0.0030451908348518098\n",
      "411 | 0.0029765234154641397\n",
      "412 | 0.0029094696289529327\n",
      "413 | 0.002843989082206808\n",
      "414 | 0.002780042480848355\n",
      "415 | 0.0027175915966141423\n",
      "416 | 0.0026565992357689876\n",
      "417 | 0.002597029208521916\n",
      "418 | 0.002538846299410355\n",
      "419 | 0.00248201623862164\n",
      "420 | 0.002426505674221763\n",
      "421 | 0.0023722821452618584\n",
      "422 | 0.002319314055732302\n",
      "423 | 0.0022675706493393466\n",
      "424 | 0.0022170219850758412\n",
      "425 | 0.0021676389135593882\n",
      "426 | 0.0021193930541147116\n",
      "427 | 0.002072256772575282\n",
      "428 | 0.0020262031597791177\n",
      "429 | 0.0019812060107382537\n",
      "430 | 0.00193723980445772\n",
      "431 | 0.0018942796843851944\n",
      "432 | 0.00185230143946696\n",
      "433 | 0.0018112814857948025\n",
      "434 | 0.001771196848820781\n",
      "435 | 0.0017320251461220018\n",
      "436 | 0.001693744570699178\n",
      "437 | 0.0016563338747895917\n",
      "438 | 0.0016197723541784461\n",
      "439 | 0.0015840398329901011\n",
      "440 | 0.001549116648948877\n",
      "441 | 0.0015149836390873366\n",
      "442 | 0.0014816221258913426\n",
      "443 | 0.0014490139038687625\n",
      "444 | 0.0014171412265220609\n",
      "445 | 0.001385986793719476\n",
      "446 | 0.0013555337394464196\n",
      "447 | 0.0013257656199251895\n",
      "448 | 0.0012966664020928318\n",
      "449 | 0.0012682204524252186\n",
      "450 | 0.0012404125260945503\n",
      "451 | 0.0012132277564502838\n",
      "452 | 0.0011866516448148256\n",
      "453 | 0.001160670050580048\n",
      "454 | 0.0011352691815987013\n",
      "455 | 0.0011104355848582882\n",
      "456 | 0.0010861561374310537\n",
      "457 | 0.0010624180376881311\n",
      "458 | 0.0010392087967723423\n",
      "459 | 0.0010165162303194103\n",
      "460 | 0.0009943284504200416\n",
      "461 | 0.0009726338578164328\n",
      "462 | 0.0009514211343238598\n",
      "463 | 0.0009306792354707393\n",
      "464 | 0.0009103973833527237\n",
      "465 | 0.0008905650596892514\n",
      "466 | 0.0008711719990807264\n",
      "467 | 0.000852208182457387\n",
      "468 | 0.0008336638307149474\n",
      "469 | 0.0008155293985305529\n",
      "470 | 0.0007977955683542153\n",
      "471 | 0.000780453244569387\n",
      "472 | 0.0007634935478173578\n",
      "473 | 0.0007469078094822108\n",
      "474 | 0.0007306875663282181\n",
      "475 | 0.0007148245552880454\n",
      "476 | 0.0006993107083957514\n",
      "477 | 0.0006841381478592968\n",
      "478 | 0.0006692991812700918\n",
      "479 | 0.0006547862969442529\n",
      "480 | 0.0006405921593920706\n",
      "481 | 0.0006267096049110485\n",
      "482 | 0.0006131316372992949\n",
      "483 | 0.0005998514236869031\n",
      "484 | 0.0005868622904785545\n",
      "485 | 0.0005741577194074727\n",
      "486 | 0.0005617313436951185\n",
      "487 | 0.0005495769443146704\n",
      "488 | 0.0005376884463541595\n",
      "489 | 0.0005260599154779102\n",
      "490 | 0.0005146855544815518\n",
      "491 | 0.0005035596999385411\n",
      "492 | 0.0004926768189367948\n",
      "493 | 0.0004820315059002885\n",
      "494 | 0.000471618479495363\n",
      "495 | 0.00046143257961828404\n",
      "496 | 0.00045146876446115877\n",
      "497 | 0.000441722107656013\n",
      "498 | 0.0004321877954915138\n",
      "499 | 0.00042286112420372534\n"
     ]
    }
   ],
   "source": [
    "# Setting up a matrix of the first 1000 users with movie ratings\n",
    "first_1000_users = np.matrix(user_by_movie.head(1000))\n",
    "\n",
    "# perform funkSVD on the matrix of the top 1000 users\n",
    "# fit to 1000 users with 3 latent features, lr of 0.005, and 500 iterations\n",
    "user_mat, movie_mat = FunkSVD(ratings_mat=ratings_mat,\n",
    "                              latent_features=3,\n",
    "                              learning_rate=0.005,\n",
    "                              iters=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`6.` Now that you have a set of predictions for each user-movie pair.  Let's answer a few questions about your results. Provide the correct values to each of the variables below, and check your solutions using the tests below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of actual ratings in the first_1000_users is 14287.\n",
      "\n",
      "The number of ratings made for user-movie pairs that didn't have ratings is 13835713\n"
     ]
    }
   ],
   "source": [
    "# How many actual ratings exist in first_1000_users\n",
    "num_ratings = np.count_nonzero(~np.isnan(first_1000_users))\n",
    "print(\"The number of actual ratings in the first_1000_users is {}.\".format(num_ratings))\n",
    "print()\n",
    "\n",
    "# How many ratings did we make for user-movie pairs that didn't have ratings\n",
    "ratings_for_missing = first_1000_users.shape[0]*first_1000_users.shape[1] - num_ratings\n",
    "print(\"The number of ratings made for user-movie pairs that didn't have ratings is {}\".format(ratings_for_missing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nice job!  Looks like you have predictions made for all the missing user-movie pairs! But I still have one question... How good are they?\n"
     ]
    }
   ],
   "source": [
    "# Test your results against the solution\n",
    "assert num_ratings == 14287, \"Oops!  The number of actual ratings doesn't quite look right.\"\n",
    "assert ratings_for_missing == 13835713, \"Oops!  The number of movie-user pairs that you made ratings for that didn't actually have ratings doesn't look right.\"\n",
    "\n",
    "# Make sure you made predictions on all the missing user-movie pairs\n",
    "preds = np.dot(user_mat, movie_mat)\n",
    "assert np.isnan(preds).sum() == 0\n",
    "print(\"Nice job!  Looks like you have predictions made for all the missing user-movie pairs! But I still have one question... How good are they?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
